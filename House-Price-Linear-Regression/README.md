# House Prediction system End-to-End Pipeline


---


## What I Learned during this project

**Correlation :** You should always know how your independent features are related with each other or with the target and  , Could be highy positive or highly negative.
        
* If the independent features are highly correleated with each other then we can always remove one of them --> known as **"Multiple correality"**

* Positive Correlation : when value of one variable increases as the other one goes up.
        Example : As the petrol price goes up , inflation(Mehangai) goes up

* Negative Correlation : when value of one variable decreases as the other one goes up.
        Example : As the petrol price goes up , use of cars decreases


There are multiple types of Method to find correlation :
1. Pearson Method : it checks how the independented and target feature are related , value ranges between -1 to 1 , if close to 1 then highly positive , if close to -1 highly negative , if in mid then they are not correleated

<br>

* **Types of Cross Validation**
--> (To check the overfitting[If our model does much better on the training set than on the test set]) We can find min , max or average accuracy we can get

1. LOOCV (Leave One Out Cross Validation) -> 1% as test , 99% as train
        Example : We have 1000 records , 
                Exp 1.  --> 1st record as test others are train and we train and get the accuracy
                Exp 2.  --> 2nd record as test others are train and we train and get the accuracy
        ,that means we have to run 1000 iterations.
    
    Disadvantages : 
    * Many iterations
    * Low Bias

2. K-fold Cross Validation
        Example : We select a number K and K signifies the no. of Experiments , again we have 1000 records , k=5 so data will be split into 1000/5= 200
                Exp 1. 0-200 as test data and rest train
                Exp 2. 200-400 as test data and rest train
                Exp 3. 400-600 as test data and rest train
                Exp 4. 600-800 as test data and rest train
                Exp 5. 800-1000 as test data and rest train
    
    Disadvantages :
    * Data can become biased

3. Stratified K-fold Cross Validation
        Example : Works same as k-fold but it make sure the data divide has some no. of instance of each class

4. Time-Series Cross Validation


<br>

* ```%matplotlib inline``` makes the plots generated by matplotlib show into the IPython shell that we are running and not in a separate output window.

* To create a better regression modelm linearity(if x changes then y also changes n vice-versa) should always be there in the dataset

<br>

* **Eucledian Distance**
(Shortest path) To check the distance between two points p1 and p2 we will use Eucledian Distance and if we find the parameters near to each other than we can say they are similar. It is calculated based on the Pythogorous Theoroem 

                                d = √[ (x2 – x1)^2 + (y2– y1)^2]

<br>

* **Manhattan Distance**
The Manhattan distance (following the pattern) as the sum of absolute differences , ManhattanDistance [{x1, y1}, {x2, y2}]

                                Abs [x2 − x1] + Abs [y2 − y1] ......

<br>

* **Methods of Feature Scaling**

1. **Standardisation or Standard Scalar** : This redistibute the features with their mean=0 and std. deviation =1 
                                          (x-mean)/std. deviation

2. **Mean Normalisation** : This redistribute the value between -1 and 1 with mean=0

                                          (x-mean)/(max-min)


3. **Min-max Scaling** : This Scales the value between 0 and 1
                                          (x-min)/(max-min)

4. **Unit Vector** :                    x/(Abs(x))

==> Any Algorithm which is not Distance based is not affected by Feature scaling.

* **Standarization VS Normalization**

<br>

* residual : is just the error between the predicted vs actual value


* **Cost function** :  How your model is performing

* mean absolute error
* mean squared error
* root mean squared error
* R square
* Adjusted R square
---

### Shortcuts

* Esc + A ---> to create more empty cell

---

### Questions

Robust Scalar ?
Is Min-max Scaler is normalization
Formula for mean square error , mean Absolute error


train-test-split : stratifyarray-like, default=None
why use correlation ?
